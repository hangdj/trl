{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "# def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "#     \"\"\"Sample text from language model.\"\"\"\n",
    "#     input_ids = queries\n",
    "#     for i in range(txt_len):\n",
    "#         # Get Logits\n",
    "#         outputs = model(input_ids)\n",
    "#         next_token_logits = outputs[0][:, -1, :]\n",
    "#         next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "#         # Sample\n",
    "#         probs = F.softmax(next_token_logits, dim=-1)\n",
    "#         next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "#         input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "#     # return input_ids[:, -txt_len:]\n",
    "#     return input_ids[:, :]  # XD\n",
    "\n",
    "# device = torch.device('cuda:0')\n",
    "# model = AutoModelWithLMHead.from_pretrained('/home/yjzhou/transformers/examples/models/gpt2_sqli_rand4000_small/')\n",
    "# _ = model.to(device)\n",
    "# _ = model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# bsz = 16\n",
    "# bos_token = '0'\n",
    "# bos_token_id = tokenizer._convert_token_to_id(bos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1065, 2310, 1315, 1467, 362, 718, 657]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1065,  2310,  1315,  1467,   362,   718,   657,   657,   657, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids=input_ids[0]\n",
    "type(ids)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 6],\n",
       "        [0, 7],\n",
       "        [0, 8]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero((ids==657))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=torch.nonzero((ids==657))[0]\n",
    "# print(idx)\n",
    "# idx=idx.repeat(1,768).reshape(1,1,768)\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  15, 1105, 2310, 1467,  362]])\n",
      "sdfsdf\n",
      "torch.Size([0, 2])\n",
      "tensor(4)\n",
      "tensor([[  15,  362,  642,  718, 1105]])\n",
      "sdfsdf\n",
      "torch.Size([0, 2])\n",
      "tensor(4)\n",
      "tensor([[  15,  718, 9919, 1679,  657]])\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "content=[\"0 12 21 16 2 6 0\",\"0 2 5 6 12 15 0\",\"0 6 89 25 0\"]#\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "indices=[]\n",
    "max_length=5\n",
    "tokenizer.pad_token =\"!\"\n",
    "    # For every sentence...\n",
    "for sent in content:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=max_length,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors='pt',  # Return pytorch tensors.\n",
    "    )\n",
    "\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    print((encoded_dict['input_ids']))\n",
    "#     print(type(encoded_dict['input_ids']))\n",
    "#     print(torch.nonzero((encoded_dict['input_ids']==657)))\n",
    "    idx=torch.nonzero((encoded_dict['input_ids']==657))\n",
    "    if idx.shape[0]==0:\n",
    "        print('sdfsdf')\n",
    "        print(idx.shape)\n",
    "        idx=torch.tensor(max_length-1)\n",
    "#     [0][1]\n",
    "    else:\n",
    "        idx=idx[0][1]\n",
    "    print(idx)\n",
    "    idx=idx.repeat(1,1)\n",
    "#     print(idx.shape)\n",
    "#     idx=idx.reshape(1,1,768)\n",
    "    indices.append(idx)\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "indices = torch.cat(indices, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  15, 1105, 2310, 1467,  362],\n",
       "         [  15,  362,  642,  718, 1105],\n",
       "         [  15,  718, 9919, 1679,  657]]),\n",
       " tensor([[4],\n",
       "         [4],\n",
       "         [4]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 1), dtype=torch.int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=torch.nonzero((input_ids[0]==657))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 5]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape,input_ids.shape,attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 3, 3, 3]],\n",
       "\n",
       "        [[6, 6, 6, 6]],\n",
       "\n",
       "        [[3, 3, 3, 3]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.repeat(1,4).reshape(indices.shape[0],1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 148 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (50257, 768)\n",
      "transformer.wpe.weight                                   (1024, 768)\n",
      "transformer.h.0.ln_1.weight                                   (768,)\n",
      "transformer.h.0.ln_1.bias                                     (768,)\n",
      "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias                              (768,)\n",
      "transformer.h.0.ln_2.weight                                   (768,)\n",
      "transformer.h.0.ln_2.bias                                     (768,)\n",
      "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
      "transformer.h.1.ln_1.weight                                   (768,)\n",
      "transformer.h.1.ln_1.bias                                     (768,)\n",
      "transformer.h.1.attn.c_attn.weight                       (768, 2304)\n",
      "transformer.h.1.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.1.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.1.attn.c_proj.bias                              (768,)\n",
      "transformer.h.1.ln_2.weight                                   (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.h.11.mlp.c_proj.weight                       (3072, 768)\n",
      "transformer.h.11.mlp.c_proj.bias                              (768,)\n",
      "transformer.ln_f.weight                                       (768,)\n",
      "transformer.ln_f.bias                                         (768,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1065,  2310,  1315,  1467,   362,   718,   657, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [   17,   642,   718,  1105,  1315,   657, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [   21,  9919,   657, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks=attention_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.transformer(input_ids,attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 50257])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][:,-1:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 50257])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.LongTensor([[bos_token_id]]).expand(bsz, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries\n",
    "from transformers.modeling_utils import top_k_top_p_filtering\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries.to(device)\n",
    "outputs = respond_to_batch(model, queries, txt_len=60, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 61])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yjzhou/transformers/examples/choice.txt','a') as f1:\n",
    "    for j in range(0,1000):\n",
    "        queries = torch.LongTensor([[bos_token_id]]).expand(bsz, 1)\n",
    "        queries = queries.to(device)\n",
    "        outputs = respond_to_batch(model, queries, txt_len=60, top_k=20)\n",
    "    \n",
    "        for i in range(len(outputs)):\n",
    "            tmpstr = tokenizer.decode(outputs[i])\n",
    "            #print(tmpstr)\n",
    "            count = tmpstr.find('0!',1)\n",
    "            #print(tmpstr[:count])\n",
    "            if count != -1:\n",
    "                f1.write(tmpstr[:count])\n",
    "                f1.write('\\n')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 2, 2, 2]],\n",
       "\n",
       "        [[1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "indices=torch.LongTensor([[2],[1],[0]]).repeat(1,4).reshape(3,1,4)\n",
    "print(indices.shape)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [2, 2, 2, 2],\n",
       "         [3, 3, 3, 3]],\n",
       "\n",
       "        [[4, 4, 4, 4],\n",
       "         [5, 5, 5, 5],\n",
       "         [6, 6, 6, 6]],\n",
       "\n",
       "        [[7, 7, 7, 7],\n",
       "         [8, 8, 8, 8],\n",
       "         [9, 9, 9, 9]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=torch.LongTensor([[[1,1,1,1],[2,2,2,2],[3,3,3,3]],[[4,4,4,4],[5,5,5,5],[6,6,6,6]],[[7,7,7,7],[8,8,8,8],[9,9,9,9]]])\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 3, 3, 3]],\n",
       "\n",
       "        [[5, 5, 5, 5]],\n",
       "\n",
       "        [[7, 7, 7, 7]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(data,1,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for path in ['/home/hdj/data/CodeBERT/eval.csv']:#,'/home/hdj/data/CodeBERT/train.csv'\n",
    "    with open(path) as f_in:\n",
    "        contents=[]\n",
    "        labels=[]\n",
    "        for line in f_in:\n",
    "\n",
    "            line=line.replace(',',' ')\n",
    "            line=line.split('||')\n",
    "\n",
    "            left=line[0].strip()\n",
    "            right=line[1].strip()\n",
    "    #         print(left,label)\n",
    "            contents.append(left)\n",
    "            labels.append(right)\n",
    "        data=pd.DataFrame()\n",
    "        data['content']=contents\n",
    "        data['label']=labels\n",
    "        data.to_csv('/home/hdj/data/CodeBERT/eval_hdj.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=pd.read_csv('/data/hdj/data/CodeBERT/eval_hdj.csv')\n",
    "val.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 2 3 108 4 110 8 84 6 59 62 9 74 6 59 63 7 33...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 1 12 3 108 4 109 6 59 63 7 33 16 82 6 59 62 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 1 12 66 102 4 109 6 59 63 7 32 8 85 6 59 63 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 2 3 108 4 110 5 70 6 59 63 10 7 32 8 84 6 59...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0 1 12 3 108 4 109 6 59 62 7 32 8 84 6 59 62 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  0 2 3 108 4 110 8 84 6 59 62 9 74 6 59 63 7 33...      0\n",
       "1  0 1 12 3 108 4 109 6 59 63 7 33 16 82 6 59 62 ...      0\n",
       "2  0 1 12 66 102 4 109 6 59 63 7 32 8 85 6 59 63 ...      0\n",
       "3  0 2 3 108 4 110 5 70 6 59 63 10 7 32 8 84 6 59...      1\n",
       "4  0 1 12 3 108 4 109 6 59 62 7 32 8 84 6 59 62 3...      0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('/data/hdj/data/CodeBERT/train_hdj.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(file_path, tokenizer, max_len=60, mode='train'):\n",
    "    '''\n",
    "    file_path: the path to input file.\n",
    "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
    "                The first row must be header of columns.\n",
    "\n",
    "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
    "                The first row must be header of column.\n",
    "\n",
    "    lab2ind: dictionary of label classes\n",
    "    tokenizer: BERT tokenizer\n",
    "    max_len: maximal length of input sequence\n",
    "    mode: train or predict\n",
    "    '''\n",
    "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
    "    if mode == 'train':\n",
    "        # Use pandas to load dataset\n",
    "        df = pd.read_csv(file_path, delimiter=',', header=0, names=['content', 'label'])\n",
    "        print(\"Data size \", df.shape)\n",
    "        labels = df.label.values\n",
    "\n",
    "        # Create sentence and label lists\n",
    "        labels = [i for i in labels]\n",
    "        print(\"Label is \", labels[0])\n",
    "\n",
    "        # Convert data into torch tensors\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # if we are in predict mode, we will load one column (i.e., text).\n",
    "    elif mode == 'predict':\n",
    "        df = pd.read_csv(file_path, delimiter=',', header=0, names=['content'])\n",
    "        print(\"Data size \", df.shape)\n",
    "        # create placeholder\n",
    "        labels = []\n",
    "    else:\n",
    "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
    "        return\n",
    "\n",
    "    # Create sentence and label lists\n",
    "    content = df.content.values\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    indices=[]\n",
    "    tokenizer.pad_token = \"!\"\n",
    "    # For every sentence...\n",
    "    for sent in content:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_len,  # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors='pt',  # Return pytorch tensors.\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        idx = torch.nonzero((encoded_dict['input_ids'] == 657))\n",
    "        if idx.shape[0]==0:\n",
    "            idx=torch.tensor(max_len-1)\n",
    "        else:\n",
    "            idx=idx[0][1]\n",
    "#         if idx==60：\n",
    "#         print(idx)\n",
    "        idx = idx.repeat(1, 1)\n",
    "#         try:\n",
    "#             idx = torch.nonzero((encoded_dict['input_ids'] == 657))[0][1]\n",
    "#             idx = idx.repeat(1, 1)\n",
    "#         except:\n",
    "#             print(content,encoded_dict['input_ids'])\n",
    "#             break\n",
    "        \n",
    "        indices.append(idx)\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    indices = torch.cat(indices, dim=0)\n",
    "#     print(\"The first sentence untokenized:\\n\", content[0])\n",
    "#     print(\"Index numbers of the first sentence after padding:\\n\", input_ids[0])\n",
    "\n",
    "    return input_ids,  attention_masks,labels,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size  (4000, 2)\n",
      "Label is  0\n"
     ]
    }
   ],
   "source": [
    "train_inputs,  train_masks,train_labels,train_indices = data_prepare(path+\"train_hdj.csv\",tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size  (4000, 2)\n",
      "Label is  0\n",
      "Data size  (674, 2)\n",
      "Label is  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path=\"/data/hdj/data/CodeBERT/\"\n",
    "# Use defined funtion to extract data\n",
    "train_inputs,  train_masks,train_labels,train_indices = data_prepare(path+\"train_hdj.csv\",tokenizer)\n",
    "validation_inputs, validation_masks ,validation_labels, val_indices= data_prepare(path+\"eval_hdj.csv\",tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 60]) torch.Size([4000, 60]) torch.Size([4000]) torch.Size([4000, 1])\n",
      "torch.Size([674, 60]) torch.Size([674, 60]) torch.Size([674]) torch.Size([674, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs.shape,train_masks.shape,train_labels.shape,train_indices.shape)\n",
    "print(validation_inputs.shape, validation_masks.shape ,validation_labels.shape, val_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels,train_indices)\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "val_dataset = TensorDataset(validation_inputs, validation_masks, validation_labels,val_indices)\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell\n",
    "\n",
    "class ValueHead(torch.nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "    def forward(self, hidden_states):\n",
    "        output = hidden_states\n",
    "#         output = self.drop(output)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# Cell\n",
    "\n",
    "class GPT2HeadWithValueModel(torch.nn.Module):\n",
    "    \"\"\"The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head.\"\"\"\n",
    "    def __init__(self, model_path,num_labels):\n",
    "        super().__init__()\n",
    "#         config.num_labels = 2\n",
    "        if(os.path.exists(model_path)):\n",
    "            self.model = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "        else:\n",
    "            print(\"error model path not exists\")\n",
    "#         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(num_labels)\n",
    "#         self.loss_fn=CrossEntropyLoss()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        indices=None,\n",
    "#         label=None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        indices=indices.repeat(1,768).reshape(indices.shape[0],1,768)\n",
    "#         print('transformer_outputs ',type(transformer_outputs),transformer_outputs[0].shape)\n",
    "#         print('indices ',type(indices),indices.shape)\n",
    "        \n",
    "        hidden_states=torch.gather(transformer_outputs[0],1,indices)\n",
    "#         print('hidden_states ',type(hidden_states),hidden_states.shape)\n",
    "        \n",
    "        logits = torch.squeeze(self.v_head(hidden_states),1) \n",
    "        return logits\n",
    "#         print('logits.shape:  ****',logits.shape)\n",
    "#         print('labels.shape: ',label.shape)\n",
    "#         print()\n",
    "#         if(label!=None):\n",
    "#             loss=self.loss_fn(logits,label)\n",
    "#         else:\n",
    "#             loss=None\n",
    "#         return loss,logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.0000],\n",
       "         [0.2000, 0.8000],\n",
       "         [0.5000, 0.5000]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.Tensor([[1,0],[0.2,0.8],[0.5,0.5]])\n",
    "input,input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1]), torch.Size([3]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(2)\n",
    "target,target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6813)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='/home/yjzhou/transformers/examples/models/gpt2_sqli_rand4000_small/'\n",
    "num_labels=2\n",
    "model=GPT2HeadWithValueModel(model_path,num_labels)\n",
    "# device = torch.device('cuda:0')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()   else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "epochs = 4\n",
    "warmup_proportion = 0.1\n",
    "num_training_steps  = len(train_dataloader) * epochs\n",
    "num_warmup_steps = num_training_steps * warmup_proportion\n",
    "\n",
    "### In Transformers, optimizer and schedules are instantiated like this:\n",
    "# Note: AdamW is a class from the huggingface library\n",
    "# the 'W' stands for 'Weight Decay\"\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "# schedules\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "\n",
    "# We use nn.CrossEntropyLoss() as our loss function.\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss,\n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60]) torch.Size([32, 60]) torch.Size([32]) torch.Size([32, 1])\n",
      "torch.Size([32, 60]) torch.Size([32, 60]) torch.Size([32]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_indices=batch[3].to(device)\n",
    "    print(b_input_ids.shape,b_input_mask.shape,b_labels.shape,b_indices.shape)\n",
    "    break\n",
    "for step, batch in enumerate(validation_dataloader):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_indices=batch[3].to(device)\n",
    "    print(b_input_ids.shape,b_input_mask.shape,b_labels.shape,b_indices.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    125.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    125.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    125.    Elapsed: 0:00:13.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.96\n",
      "  Validation Loss: 0.12\n",
      "  Validation took: 0:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    125.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    125.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    125.    Elapsed: 0:00:12.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.98\n",
      "  Validation Loss: 0.05\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    125.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    125.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    125.    Elapsed: 0:00:12.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.04\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    125.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    125.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    125.    Elapsed: 0:00:12.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epcoh took: 0:00:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.99\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:13 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_indices=batch[3].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "        loss_fn=CrossEntropyLoss()\n",
    "        loss=loss_fn(logits,b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "#         break\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_indices = batch[3].to(device)\n",
    "        with torch.no_grad():\n",
    "#             loss, logits = model(b_input_ids,attention_mask=b_input_mask,label=b_labels,indices=b_indices)\n",
    "            logits = model(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "            loss_fn=CrossEntropyLoss()\n",
    "            loss=loss_fn(logits,b_labels)\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    # Create checkpoint at end of each epoch\n",
    "    state = {\n",
    "        'epoch': epoch_i,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, \"\" + str(epoch_i + 1) + \".pt\")\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "        'epoch': epoch_i,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "torch.save(state, \"\" + str(epoch_i + 1) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint!\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model, checkpoint_PATH, optimizer):\n",
    "    if checkpoint_PATH != None:\n",
    "        model_CKPT = torch.load(checkpoint_PATH)\n",
    "        model.load_state_dict(model_CKPT['state_dict'])\n",
    "        print('loading checkpoint!')\n",
    "        optimizer.load_state_dict(model_CKPT['optimizer'])\n",
    "    return model, optimizer\n",
    "model_load=GPT2HeadWithValueModel(\"/home/yjzhou/transformers/examples/models/gpt2_sqli_rand4000_small/\",num_labels)\n",
    "optimizer_load = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "# schedules\n",
    "scheduler_load = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "\n",
    "model_load,optimizer_load=load_checkpoint(model_load,'/home/hdj/trl/nbs/4.pt',optimizer_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.2644691658988595,\n",
       "  'Valid. Loss': 0.15922592911043798,\n",
       "  'Valid. Accur.': 0.9431818181818182,\n",
       "  'Training Time': '0:04:30',\n",
       "  'Validation Time': '0:00:09'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.10734188155736774,\n",
       "  'Valid. Loss': 0.11009371190712872,\n",
       "  'Valid. Accur.': 0.9673295454545454,\n",
       "  'Training Time': '0:04:32',\n",
       "  'Validation Time': '0:00:10'},\n",
       " {'epoch': 3,\n",
       "  'Training Loss': 0.05057939318264835,\n",
       "  'Valid. Loss': 0.04962746564739395,\n",
       "  'Valid. Accur.': 0.9872159090909091,\n",
       "  'Training Time': '0:04:36',\n",
       "  'Validation Time': '0:00:10'},\n",
       " {'epoch': 4,\n",
       "  'Training Loss': 0.033867501211352645,\n",
       "  'Valid. Loss': 0.042183015490247104,\n",
       "  'Valid. Accur.': 0.9900568181818182,\n",
       "  'Training Time': '0:04:36',\n",
       "  'Validation Time': '0:00:09'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'epoch': 1,\n",
    "  'Training Loss': 1.0033449912071228,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:23',\n",
    "  'Validation Time': '0:00:10'},\n",
    " {'epoch': 2,\n",
    "  'Training Loss': 0.9981416406631469,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:26',\n",
    "  'Validation Time': '0:00:10'},\n",
    " {'epoch': 3,\n",
    "  'Training Loss': 0.9991825561523437,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:24',\n",
    "  'Validation Time': '0:00:10'},\n",
    " {'epoch': 4,\n",
    "  'Training Loss': 0.9964690442085267,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:27',\n",
    "  'Validation Time': '0:00:10'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
