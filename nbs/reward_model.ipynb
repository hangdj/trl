{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "def respond_to_batch(model, queries, txt_len=20, top_k=0, top_p=1.0):\n",
    "    \"\"\"Sample text from language model.\"\"\"\n",
    "    input_ids = queries\n",
    "    for i in range(txt_len):\n",
    "        # Get Logits\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs[0][:, -1, :]\n",
    "        next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "        # Sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "    # return input_ids[:, -txt_len:]\n",
    "    return input_ids[:, :]  # XD\n",
    "\n",
    "# device = torch.device('cuda:0')\n",
    "# model = AutoModelWithLMHead.from_pretrained('/home/yjzhou/transformers/examples/models/gpt2_sqli_rand4000_small/')\n",
    "# _ = model.to(device)\n",
    "# _ = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "bsz = 16\n",
    "bos_token = '0'\n",
    "bos_token_id = tokenizer._convert_token_to_id(bos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1065, 2310, 1315, 1467, 362, 718, 657]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx=torch.nonzero((ids==657))[0]\n",
    "# print(idx)\n",
    "# idx=idx.repeat(1,768).reshape(1,1,768)\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bos_token_id = tokenizer._convert_token_to_id(' !')\n",
    "bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   15,   362,   642,   657, 10185]])\n",
      "tensor(3)\n",
      "tensor([[ 15, 362, 642, 657,   0]])\n",
      "tensor(3)\n",
      "tensor([[  15,  657, 3228,    0,    0]])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "content=[\"0 2 5 0!!!\",\"0 2 5 0! \",\"0 0!!\"]\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "indices=[]\n",
    "max_length=5\n",
    "tokenizer.pad_token =\"!\"\n",
    "    # For every sentence...\n",
    "for sent in content:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=max_length,  # Pad & truncate all sentences.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Construct attn. masks.\n",
    "        return_tensors='pt',  # Return pytorch tensors.\n",
    "    )\n",
    "    # Add the encoded sentence to the list.\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    print((encoded_dict['input_ids']))\n",
    "#     print(type(encoded_dict['input_ids']))\n",
    "#     print(torch.nonzero((encoded_dict['input_ids']==657)))\n",
    "    idx=torch.nonzero((encoded_dict['input_ids']==657))\n",
    "    if idx.shape[0]==0:\n",
    "        print('sdfsdf')\n",
    "        print(idx.shape)\n",
    "        idx=torch.tensor(max_length-1)\n",
    "#     [0][1]\n",
    "    else:\n",
    "        idx=idx[0][1]\n",
    "    print(idx)\n",
    "    idx=idx.repeat(1,1)\n",
    "#     print(idx.shape)\n",
    "#     idx=idx.reshape(1,1,768)\n",
    "    indices.append(idx)\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "indices = torch.cat(indices, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  15, 1105, 2310, 1467,  362],\n",
       "         [  15,  362,  642,  718, 1105],\n",
       "         [  15,  718, 9919, 1679,  657]]),\n",
       " tensor([[4],\n",
       "         [4],\n",
       "         [4]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 1), dtype=torch.int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=torch.nonzero((input_ids[0]==657))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 5]), torch.Size([1, 5]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape,input_ids.shape,attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 3, 3, 3]],\n",
       "\n",
       "        [[6, 6, 6, 6]],\n",
       "\n",
       "        [[3, 3, 3, 3]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.repeat(1,4).reshape(indices.shape[0],1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 148 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (50257, 768)\n",
      "transformer.wpe.weight                                   (1024, 768)\n",
      "transformer.h.0.ln_1.weight                                   (768,)\n",
      "transformer.h.0.ln_1.bias                                     (768,)\n",
      "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias                              (768,)\n",
      "transformer.h.0.ln_2.weight                                   (768,)\n",
      "transformer.h.0.ln_2.bias                                     (768,)\n",
      "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
      "transformer.h.1.ln_1.weight                                   (768,)\n",
      "transformer.h.1.ln_1.bias                                     (768,)\n",
      "transformer.h.1.attn.c_attn.weight                       (768, 2304)\n",
      "transformer.h.1.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.1.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.1.attn.c_proj.bias                              (768,)\n",
      "transformer.h.1.ln_2.weight                                   (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.h.11.mlp.c_proj.weight                       (3072, 768)\n",
      "transformer.h.11.mlp.c_proj.bias                              (768,)\n",
      "transformer.ln_f.weight                                       (768,)\n",
      "transformer.ln_f.bias                                         (768,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1065,  2310,  1315,  1467,   362,   718,   657, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [   17,   642,   718,  1105,  1315,   657, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [   21,  9919,   657, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks=attention_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.transformer(input_ids,attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 50257])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][:,-1:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 50257])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.LongTensor([[bos_token_id]]).expand(bsz, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries\n",
    "from transformers.modeling_utils import top_k_top_p_filtering\n",
    "from torch.nn import Identity\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries.to(device)\n",
    "outputs = respond_to_batch(model, queries, txt_len=60, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 61])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yjzhou/transformers/examples/choice.txt','a') as f1:\n",
    "    for j in range(0,1000):\n",
    "        queries = torch.LongTensor([[bos_token_id]]).expand(bsz, 1)\n",
    "        queries = queries.to(device)\n",
    "        outputs = respond_to_batch(model, queries, txt_len=60, top_k=20)\n",
    "    \n",
    "        for i in range(len(outputs)):\n",
    "            tmpstr = tokenizer.decode(outputs[i])\n",
    "            #print(tmpstr)\n",
    "            count = tmpstr.find('0!',1)\n",
    "            #print(tmpstr[:count])\n",
    "            if count != -1:\n",
    "                f1.write(tmpstr[:count])\n",
    "                f1.write('\\n')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 2, 2, 2]],\n",
       "\n",
       "        [[1, 1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "indices=torch.LongTensor([[2],[1],[0]]).repeat(1,4).reshape(3,1,4)\n",
    "print(indices.shape)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [2, 2, 2, 2],\n",
       "         [3, 3, 3, 3]],\n",
       "\n",
       "        [[4, 4, 4, 4],\n",
       "         [5, 5, 5, 5],\n",
       "         [6, 6, 6, 6]],\n",
       "\n",
       "        [[7, 7, 7, 7],\n",
       "         [8, 8, 8, 8],\n",
       "         [9, 9, 9, 9]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=torch.LongTensor([[[1,1,1,1],[2,2,2,2],[3,3,3,3]],[[4,4,4,4],[5,5,5,5],[6,6,6,6]],[[7,7,7,7],[8,8,8,8],[9,9,9,9]]])\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3, 3, 3, 3]],\n",
       "\n",
       "        [[5, 5, 5, 5]],\n",
       "\n",
       "        [[7, 7, 7, 7]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(data,1,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始的地方********\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "train=pd.read_csv('/data/hdj/data/CodeBERT/train_0206.csv')\n",
    "# data.head()\n",
    "val=pd.read_csv('/data/hdj/data/CodeBERT/val_0206.csv')\n",
    "test=pd.read_csv('/data/hdj/data/CodeBERT/test_0206.csv')\n",
    "# test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 2 3 108 4 110 8 84 6 59 62 9 77 6 59 63 7 33...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 2 3 108 4 110 5 70 6 59 62 10 7 32 8 85 6 59...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 2 3 108 4 110 5 72 6 59 62 7 33 16 83 6 59 6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 1 12 3 108 4 109 6 59 63 7 33 16 82 6 59 63 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0 1 12 66 104 4 110 6 59 63 7 33 16 83 6 59 62...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12955</th>\n",
       "      <td>0 1 12 3 108 6 59 63 7 32 8 84 6 59 63 34 43 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12956</th>\n",
       "      <td>0 2 3 108 4 110 5 68 6 59 62 7 32 8 85 6 59 63...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12957</th>\n",
       "      <td>0 2 3 108 4 109 5 72 6 59 62 10 7 33 16 82 6 5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12958</th>\n",
       "      <td>0 2 3 108 4 110 8 84 6 59 62 9 75 6 59 63 7 32...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12959</th>\n",
       "      <td>0 1 12 66 102 4 110 6 59 62 7 33 16 82 6 59 63...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label\n",
       "0      0 2 3 108 4 110 8 84 6 59 62 9 77 6 59 63 7 33...      0\n",
       "1      0 2 3 108 4 110 5 70 6 59 62 10 7 32 8 85 6 59...      1\n",
       "2      0 2 3 108 4 110 5 72 6 59 62 7 33 16 83 6 59 6...      0\n",
       "3      0 1 12 3 108 4 109 6 59 63 7 33 16 82 6 59 63 ...      0\n",
       "4      0 1 12 66 104 4 110 6 59 63 7 33 16 83 6 59 62...      0\n",
       "...                                                  ...    ...\n",
       "12955  0 1 12 3 108 6 59 63 7 32 8 84 6 59 63 34 43 3...      0\n",
       "12956  0 2 3 108 4 110 5 68 6 59 62 7 32 8 85 6 59 63...      1\n",
       "12957  0 2 3 108 4 109 5 72 6 59 62 10 7 33 16 82 6 5...      0\n",
       "12958  0 2 3 108 4 110 8 84 6 59 62 9 75 6 59 63 7 32...      0\n",
       "12959  0 1 12 66 102 4 110 6 59 62 7 33 16 82 6 59 63...      0\n",
       "\n",
       "[12960 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12960, 2),\n",
       " (3240, 2),\n",
       " (4050, 2),\n",
       " 0.1705246913580247,\n",
       " 0.17037037037037037,\n",
       " 0.17037037037037037)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,val.shape,test.shape,sum(train['label'].values)/train.shape[0],sum(val['label'].values)/val.shape[0],sum(test['label'].values)/test.shape[0],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.tensor([1,2,3])\n",
    "index=[0,1]\n",
    "# a[index]>0\n",
    "filterIndex=[]\n",
    "for ind in index:\n",
    "    if a[ind]>0:\n",
    "        filterIndex.append(ind)\n",
    "filterIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(file_path, tokenizer, max_len=66, mode='train'):\n",
    "    '''\n",
    "    file_path: the path to input file.\n",
    "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
    "                The first row must be header of columns.\n",
    "\n",
    "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
    "                The first row must be header of column.\n",
    "\n",
    "    lab2ind: dictionary of label classes\n",
    "    tokenizer: BERT tokenizer\n",
    "    max_len: maximal length of input sequence\n",
    "    mode: train or predict\n",
    "    '''\n",
    "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
    "    if mode == 'train':\n",
    "        # Use pandas to load dataset\n",
    "        df = pd.read_csv(file_path, delimiter=',', header=0, names=['content', 'label'])\n",
    "        print(\"Data size \", df.shape)\n",
    "        labels = df.label.values\n",
    "\n",
    "        # Create sentence and label lists\n",
    "        labels = [i for i in labels]\n",
    "        print(\"Label is \", labels[0])\n",
    "\n",
    "        # Convert data into torch tensors\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    # if we are in predict mode, we will load one column (i.e., text).\n",
    "    elif mode == 'predict':\n",
    "        df = pd.read_csv(file_path, delimiter=',', header=0, names=['content'])\n",
    "        print(\"Data size \", df.shape)\n",
    "        # create placeholder\n",
    "        labels = []\n",
    "    else:\n",
    "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
    "        return\n",
    "\n",
    "    # Create sentence and label lists\n",
    "    content = df.content.values\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    indices=[]\n",
    "    tokenizer.pad_token = \"!\"\n",
    "    # For every sentence...\n",
    "    for sent in content:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=max_len,  # Pad & truncate all sentences.\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,  # Construct attn. masks.\n",
    "            return_tensors='pt',  # Return pytorch tensors.\n",
    "        )\n",
    "        \n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        idx = torch.nonzero((encoded_dict['input_ids'] == 657))\n",
    "        if idx.shape[0]==0:\n",
    "            idx=torch.tensor(max_len-1)\n",
    "        else:\n",
    "            idx=idx[0][1]\n",
    "#         if idx==60：\n",
    "#         print(idx)\n",
    "        idx = idx.repeat(1, 1)\n",
    "#         try:\n",
    "#             idx = torch.nonzero((encoded_dict['input_ids'] == 657))[0][1]\n",
    "#             idx = idx.repeat(1, 1)\n",
    "#         except:\n",
    "#             print(content,encoded_dict['input_ids'])\n",
    "#             break\n",
    "        \n",
    "        indices.append(idx)\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    indices = torch.cat(indices, dim=0)\n",
    "#     print(\"The first sentence untokenized:\\n\", content[0])\n",
    "#     print(\"Index numbers of the first sentence after padding:\\n\", input_ids[0])\n",
    "\n",
    "    return input_ids,  attention_masks,labels,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size  (12960, 2)\n",
      "Label is  0\n",
      "Data size  (3240, 2)\n",
      "Label is  0\n",
      "Data size  (4050, 2)\n",
      "Label is  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path=\"/data/hdj/data/CodeBERT/\"\n",
    "# Use defined funtion to extract data\n",
    "train_inputs,  train_masks,train_labels,train_indices = data_prepare(path+\"train_0206.csv\",tokenizer)\n",
    "validation_inputs, validation_masks ,validation_labels, val_indices= data_prepare(path+\"val_0206.csv\",tokenizer)\n",
    "test_inputs,  test_masks,test_labels,test_indices = data_prepare(path+\"test_0206.csv\",tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12960, 66]) torch.Size([12960, 66]) torch.Size([12960]) torch.Size([12960, 1])\n",
      "torch.Size([3240, 66]) torch.Size([3240, 66]) torch.Size([3240]) torch.Size([3240, 1])\n",
      "torch.Size([4050, 66]) torch.Size([4050, 66]) torch.Size([4050]) torch.Size([4050, 1])\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs.shape,train_masks.shape,train_labels.shape,train_indices.shape)\n",
    "print(validation_inputs.shape, validation_masks.shape ,validation_labels.shape, val_indices.shape)\n",
    "print(test_inputs.shape, test_masks.shape ,test_labels.shape, test_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels,train_indices)\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "val_dataset = TensorDataset(validation_inputs, validation_masks, validation_labels,val_indices)\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels,test_indices)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell\n",
    "\n",
    "class ValueHead(torch.nn.Module):\n",
    "    \"\"\"The ValueHead class implements a head for GPT2 that returns a scalar for each output token.\"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "    def forward(self, hidden_states):\n",
    "        output = hidden_states\n",
    "#         output = self.drop(output)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "\n",
    "# Cell\n",
    "\n",
    "class SentimentModel(torch.nn.Module):\n",
    "    \"\"\"The GPT2HeadWithValueModel class implements a GPT2 language model with a secondary, scalar head.\"\"\"\n",
    "    def __init__(self, model_path,num_labels):\n",
    "        super().__init__()\n",
    "#         config.num_labels = 2\n",
    "        if(os.path.exists(model_path)):\n",
    "            self.model = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "        else:\n",
    "            print(\"error model path not exists\")\n",
    "#         self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.v_head = ValueHead(num_labels)\n",
    "#         self.loss_fn=CrossEntropyLoss()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        indices=None,\n",
    "#         label=None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        indices=indices.repeat(1,768).reshape(indices.shape[0],1,768)\n",
    "#         print('transformer_outputs ',type(transformer_outputs),transformer_outputs[0].shape)\n",
    "#         print('indices ',type(indices),indices.shape)\n",
    "        \n",
    "        hidden_states=torch.gather(transformer_outputs[0],1,indices)\n",
    "#         print('hidden_states ',type(hidden_states),hidden_states.shape)\n",
    "        \n",
    "        logits = torch.squeeze(self.v_head(hidden_states),1) \n",
    "        return logits\n",
    "#         print('logits.shape:  ****',logits.shape)\n",
    "#         print('labels.shape: ',label.shape)\n",
    "#         print()\n",
    "#         if(label!=None):\n",
    "#             loss=self.loss_fn(logits,label)\n",
    "#         else:\n",
    "#             loss=None\n",
    "#         return loss,logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.0000],\n",
       "         [0.2000, 0.8000],\n",
       "         [0.5000, 0.5000]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.Tensor([[1,0],[0.2,0.8],[0.5,0.5]])\n",
    "input,input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1]), torch.Size([3]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.empty(3, dtype=torch.long).random_(2)\n",
    "target,target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6813)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='/data/yjzhou/bigdatamodel2/'\n",
    "# model_path='/home/yjzhou/transformers/examples/models/gpt2_sqli_rand4000_small/'\n",
    "\n",
    "num_labels=2\n",
    "model=SentimentModel(model_path,num_labels)\n",
    "# device = torch.device('cuda:0')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()   else \"cpu\")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 2e-5\n",
    "max_grad_norm = 1.0\n",
    "epochs = 4\n",
    "warmup_proportion = 0.1\n",
    "num_training_steps  = len(train_dataloader) * epochs\n",
    "num_warmup_steps = num_training_steps * warmup_proportion\n",
    "\n",
    "### In Transformers, optimizer and schedules are instantiated like this:\n",
    "# Note: AdamW is a class from the huggingface library\n",
    "# the 'W' stands for 'Weight Decay\"\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "# schedules\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "\n",
    "# We use nn.CrossEntropyLoss() as our loss function.\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss,\n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60]) torch.Size([32, 60]) torch.Size([32]) torch.Size([32, 1])\n",
      "torch.Size([32, 60]) torch.Size([32, 60]) torch.Size([32]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_indices=batch[3].to(device)\n",
    "    print(b_input_ids.shape,b_input_mask.shape,b_labels.shape,b_indices.shape)\n",
    "    break\n",
    "for step, batch in enumerate(validation_dataloader):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_indices=batch[3].to(device)\n",
    "    print(b_input_ids.shape,b_input_mask.shape,b_labels.shape,b_indices.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "def compute_metrics(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "   \n",
    "    return acc_and_f1(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of    405.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:12.\n",
      "  Batch   160  of    405.    Elapsed: 0:00:16.\n",
      "  Batch   200  of    405.    Elapsed: 0:00:21.\n",
      "  Batch   240  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   280  of    405.    Elapsed: 0:00:29.\n",
      "  Batch   320  of    405.    Elapsed: 0:00:33.\n",
      "  Batch   360  of    405.    Elapsed: 0:00:37.\n",
      "  Batch   400  of    405.    Elapsed: 0:00:41.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:00:41\n",
      "\n",
      "Running Validation...\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9580592105263158\n",
      "\n",
      "f1 = 0.9473684210526316\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.90625\n",
      "\n",
      "acc_and_f1 = 0.853125\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.90625\n",
      "\n",
      "acc_and_f1 = 0.8377403846153846\n",
      "\n",
      "f1 = 0.7692307692307693\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8854166666666666\n",
      "\n",
      "f1 = 0.8333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.86875\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8854166666666666\n",
      "\n",
      "f1 = 0.8333333333333333\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8973214285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.86875\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.84375\n",
      "\n",
      "f1 = 0.7499999999999999\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9549632352941176\n",
      "\n",
      "f1 = 0.9411764705882353\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.84375\n",
      "\n",
      "f1 = 0.7499999999999999\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "  Accuracy: 0.98\n",
      "  total_eval_acc: 0.98\n",
      "  total_eval_acc_and_f1: 0.97\n",
      "  total_eval_f1: 0.95\n",
      "  Validation Loss: 0.05\n",
      "  Validation took: 0:00:03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdj/anaconda3/envs/waf/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:12.\n",
      "  Batch   160  of    405.    Elapsed: 0:00:16.\n",
      "  Batch   200  of    405.    Elapsed: 0:00:20.\n",
      "  Batch   240  of    405.    Elapsed: 0:00:24.\n",
      "  Batch   280  of    405.    Elapsed: 0:00:29.\n",
      "  Batch   320  of    405.    Elapsed: 0:00:33.\n",
      "  Batch   360  of    405.    Elapsed: 0:00:37.\n",
      "  Batch   400  of    405.    Elapsed: 0:00:41.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:00:41\n",
      "\n",
      "Running Validation...\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8687500000000001\n",
      "\n",
      "f1 = 0.8000000000000002\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "  Accuracy: 0.99\n",
      "  total_eval_acc: 0.99\n",
      "  total_eval_acc_and_f1: 0.98\n",
      "  total_eval_f1: 0.98\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:12.\n",
      "  Batch   160  of    405.    Elapsed: 0:00:16.\n",
      "  Batch   200  of    405.    Elapsed: 0:00:20.\n",
      "  Batch   240  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   280  of    405.    Elapsed: 0:00:29.\n",
      "  Batch   320  of    405.    Elapsed: 0:00:33.\n",
      "  Batch   360  of    405.    Elapsed: 0:00:37.\n",
      "  Batch   400  of    405.    Elapsed: 0:00:41.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epcoh took: 0:00:42\n",
      "\n",
      "Running Validation...\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9580592105263158\n",
      "\n",
      "f1 = 0.9473684210526316\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8020833333333333\n",
      "\n",
      "f1 = 0.6666666666666666\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8854166666666666\n",
      "\n",
      "f1 = 0.8333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.9375\n",
      "\n",
      "acc_and_f1 = 0.8854166666666666\n",
      "\n",
      "f1 = 0.8333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9605654761904762\n",
      "\n",
      "f1 = 0.9523809523809523\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.8177083333333333\n",
      "\n",
      "f1 = 0.6666666666666666\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9459134615384615\n",
      "\n",
      "f1 = 0.923076923076923\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "  Accuracy: 0.99\n",
      "  total_eval_acc: 0.99\n",
      "  total_eval_acc_and_f1: 0.98\n",
      "  total_eval_f1: 0.97\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:04.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:08.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:12.\n",
      "  Batch   160  of    405.    Elapsed: 0:00:16.\n",
      "  Batch   200  of    405.    Elapsed: 0:00:21.\n",
      "  Batch   240  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   280  of    405.    Elapsed: 0:00:29.\n",
      "  Batch   320  of    405.    Elapsed: 0:00:33.\n",
      "  Batch   360  of    405.    Elapsed: 0:00:37.\n",
      "  Batch   400  of    405.    Elapsed: 0:00:41.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epcoh took: 0:00:42\n",
      "\n",
      "Running Validation...\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.884375\n",
      "\n",
      "f1 = 0.8\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9129464285714286\n",
      "\n",
      "f1 = 0.8571428571428571\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9288194444444444\n",
      "\n",
      "f1 = 0.888888888888889\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9510416666666667\n",
      "\n",
      "f1 = 0.9333333333333333\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 0.96875\n",
      "\n",
      "acc_and_f1 = 0.9389204545454546\n",
      "\n",
      "f1 = 0.9090909090909091\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "acc = 1.0\n",
      "\n",
      "acc_and_f1 = 1.0\n",
      "\n",
      "f1 = 1.0\n",
      "\n",
      "  Accuracy: 1.00\n",
      "  total_eval_acc: 1.00\n",
      "  total_eval_acc_and_f1: 0.99\n",
      "  total_eval_f1: 0.99\n",
      "  Validation Loss: 0.01\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:03:26 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_indices=batch[3].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "        loss_fn=CrossEntropyLoss()\n",
    "        loss=loss_fn(logits,b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "#         break\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    total_eval_acc=0\n",
    "    total_eval_acc_and_f1=0\n",
    "    total_eval_f1=0\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_indices = batch[3].to(device)\n",
    "        with torch.no_grad():\n",
    "#             loss, logits = model(b_input_ids,attention_mask=b_input_mask,label=b_labels,indices=b_indices)\n",
    "            logits = model(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "            loss_fn=CrossEntropyLoss()\n",
    "            loss=loss_fn(logits,b_labels)\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        preds_label = np.argmax(logits, axis=1)\n",
    "        result = compute_metrics( preds_label, label_ids)\n",
    "#         for key in sorted(result.keys()):\n",
    "#             print(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        total_eval_acc+=result['acc']\n",
    "        total_eval_acc_and_f1+=result['acc_and_f1']\n",
    "        total_eval_f1+=result['f1']\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    \n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    print(\"  total_eval_acc: {0:.2f}\".format(total_eval_acc/len(validation_dataloader)))\n",
    "    print(\"  total_eval_acc_and_f1: {0:.2f}\".format(total_eval_acc_and_f1/len(validation_dataloader)))\n",
    "    print(\"  total_eval_f1: {0:.2f}\".format(total_eval_f1/len(validation_dataloader)))\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    # Create checkpoint at end of each epoch\n",
    "    state = {\n",
    "        'epoch': epoch_i,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(state, \"\" + str(epoch_i + 1) + \".pt\")\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {\n",
    "#         'epoch': epoch_i,\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'optimizer': optimizer.state_dict(),\n",
    "#         'scheduler': scheduler.state_dict()\n",
    "#     }\n",
    "# torch.save(state, \"\" + str(epoch_i + 1) + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和加载整个模型\n",
    "torch.save(model, 'model.pkl')\n",
    "model_test = torch.load('model.pkl')\n",
    "# 仅保存和加载模型参数(推荐使用)\n",
    "# torch.save(model_object.state_dict(), 'params.pkl')\n",
    "# model_object.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint!\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(model, checkpoint_PATH, optimizer):\n",
    "    if checkpoint_PATH != None:\n",
    "        model_CKPT = torch.load(checkpoint_PATH)\n",
    "        model.load_state_dict(model_CKPT['state_dict'])\n",
    "        print('loading checkpoint!')\n",
    "        optimizer.load_state_dict(model_CKPT['optimizer'])\n",
    "    return model, optimizer\n",
    "model_load=GPT2HeadWithValueModel(\"/data/yjzhou/bigdatamodel/\",num_labels)\n",
    "optimizer_load = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
    "# schedules\n",
    "scheduler_load = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "\n",
    "model,optimizer_load=load_checkpoint(model_load,'/home/hdj/trl/nbs/4.pt',optimizer_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs,  train_masks,train_labels,train_indices =\n",
    "# validation_inputs, validation_masks ,validation_labels, val_indices\n",
    "# data_inputs=train_inputs+validation_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs=torch.cat((train_inputs,validation_inputs),0)\n",
    "data_masks=torch.cat((train_masks,validation_masks),0)\n",
    "data_labels=torch.cat((train_labels,validation_labels),0)\n",
    "data_indices=torch.cat((train_indices,val_indices),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16200, 66]),\n",
       " torch.Size([16200, 66]),\n",
       " torch.Size([16200]),\n",
       " torch.Size([16200, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inputs.shape,data_masks.shape,data_labels.shape,data_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order.\n",
    "data_dataset = TensorDataset(data_inputs, data_masks, data_labels,data_indices)\n",
    "data_dataloader = DataLoader(\n",
    "            data_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(data_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:12.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:37.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:00:39\n",
      "\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:37.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:00:40\n",
      "\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:38.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:00:40\n",
      "\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    405.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    405.    Elapsed: 0:00:25.\n",
      "  Batch   120  of    405.    Elapsed: 0:00:38.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epcoh took: 0:00:40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#用完整的数据训练\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(data_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 128 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_indices=batch[3].to(device)\n",
    "        model.zero_grad()\n",
    "        logits = model(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "        loss_fn=CrossEntropyLoss()\n",
    "        loss=loss_fn(logits,b_labels)\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "#         break\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和加载整个模型\n",
    "torch.save(model, 'model.pkl')\n",
    "model_test = torch.load('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.99\n",
      "  total_eval_acc: 0.99\n",
      "  total_eval_acc_and_f1: 0.99 125.65736173548675 127\n",
      "  total_eval_f1: 0.98\n",
      "  Validation Loss: 0.02\n",
      "  Validation took: 0:00:51\n"
     ]
    }
   ],
   "source": [
    "# Tracking variables\n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "total_eval_acc=0\n",
    "total_eval_f1_acc=0\n",
    "total_eval_f1=0\n",
    "eval_f1_acc=[]\n",
    "total_eval_acc_and_f1=0\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    model_test.eval()\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_indices = batch[3].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model_test(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "        loss_fn=CrossEntropyLoss()\n",
    "        loss=loss_fn(logits,b_labels)\n",
    "    # Accumulate the validation loss.\n",
    "    total_eval_loss += loss.item()\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    preds_label = np.argmax(logits, axis=1)\n",
    "    result = compute_metrics( preds_label, label_ids)\n",
    "#     for key in sorted(result.keys()):\n",
    "#         print(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    total_eval_acc+=result['acc']\n",
    "    total_eval_acc_and_f1+=result['acc_and_f1']\n",
    "    eval_f1_acc.append(result['acc_and_f1'])\n",
    "    total_eval_f1+=result['f1']\n",
    "    # Calculate the accuracy for this batch of test sentences, and\n",
    "    # accumulate it over all batches.\n",
    "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "# Report the final accuracy for this validation run.\n",
    "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "print(\"  total_eval_acc: {0:.2f}\".format(total_eval_acc/len(test_dataloader)))\n",
    "print(\"  total_eval_acc_and_f1: {0:.2f}\".format(total_eval_acc_and_f1/len(test_dataloader)),total_eval_acc_and_f1,len(test_dataloader))\n",
    "print(\"  total_eval_f1: {0:.2f}\".format(total_eval_f1/len(test_dataloader)))\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "# Measure how long the validation run took.\n",
    "validation_time = format_time(time.time() - t0)\n",
    "print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "print(\"  Validation took: {:}\".format(validation_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size  (15991, 2)\n",
      "Label is  0\n"
     ]
    }
   ],
   "source": [
    "#在model生成的样本上测试打分的分布\n",
    "model_test_inputs,  model_test_masks,model_test_labels,model_test_indices = data_prepare(path+\"dataset15991.csv\",tokenizer)\n",
    "model_test_dataset = TensorDataset(model_test_inputs, model_test_masks, model_test_labels,model_test_indices)\n",
    "model_test_dataloader = DataLoader(\n",
    "            model_test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(model_test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[1,2,3]\n",
    "d=[4,5]\n",
    "s+=d\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.98\n",
      "  total_eval_acc: 0.98\n",
      "  total_eval_acc_and_f1: 0.97 123.56935987599041 127\n",
      "  total_eval_f1: 0.97\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:12:40\n"
     ]
    }
   ],
   "source": [
    "# Tracking variables\n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "total_eval_acc=0\n",
    "total_eval_f1_acc=0\n",
    "total_eval_f1=0\n",
    "eval_f1_acc=[]\n",
    "total_eval_acc_and_f1=0\n",
    "scores=[]\n",
    "# Evaluate data for one epoch\n",
    "for batch in model_test_dataloader:\n",
    "    model_test.eval()\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_indices = batch[3].to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model_test(b_input_ids,attention_mask=b_input_mask,indices=b_indices)\n",
    "        loss_fn=CrossEntropyLoss()\n",
    "        loss=loss_fn(logits,b_labels)\n",
    "    # Accumulate the validation loss.\n",
    "    total_eval_loss += loss.item()\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    scores+=list(logits[:,1])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    preds_label = np.argmax(logits, axis=1)\n",
    "    result = compute_metrics( preds_label, label_ids)\n",
    "#     for key in sorted(result.keys()):\n",
    "#         print(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    total_eval_acc+=result['acc']\n",
    "    total_eval_acc_and_f1+=result['acc_and_f1']\n",
    "    eval_f1_acc.append(result['acc_and_f1'])\n",
    "    total_eval_f1+=result['f1']\n",
    "    # Calculate the accuracy for this batch of test sentences, and\n",
    "    # accumulate it over all batches.\n",
    "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "# Report the final accuracy for this validation run.\n",
    "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "print(\"  total_eval_acc: {0:.2f}\".format(total_eval_acc/len(test_dataloader)))\n",
    "print(\"  total_eval_acc_and_f1: {0:.2f}\".format(total_eval_acc_and_f1/len(test_dataloader)),total_eval_acc_and_f1,len(test_dataloader))\n",
    "print(\"  total_eval_f1: {0:.2f}\".format(total_eval_f1/len(test_dataloader)))\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "# Measure how long the validation run took.\n",
    "validation_time = format_time(time.time() - t0)\n",
    "print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "print(\"  Validation took: {:}\".format(validation_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd2ddb03a90>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#对score进行排序 画出分布图\n",
    "scores=sorted(scores)\n",
    "len(scores),scores[:10]\n",
    "x=[i for i in range(len(scores))]\n",
    "plt.plot(x, scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=np.array([-8,-20,5,1,2,-3,3,0,-90,-89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 1, 0, 5, 7, 3, 4, 6, 2]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_index = sorted(range(len(rewards)), key=lambda k: rewards[k],reverse=False)  # 升序排序 reversed=False\n",
    "rewards_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 6]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_index[-4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-90, -89, -20, -8, -3, 0, 1, 2, 3, 5]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_sorted=sorted(rewards)\n",
    "rewards_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "posNum=np.sum(rewards>=0)\n",
    "if posNum*2<len(rewards):#做数据均衡处理\n",
    "    \n",
    "    largeNegNum=int(posNum*0.8)#采样数量\n",
    "    print('largeRotioNeg :',largeNegNum)\n",
    "    smallNegNum=posNum-largeNegNum#采样数量\n",
    "    print('smallRotioNeg :',smallNegNum)\n",
    "    choiceSmall=rewards_index[:len(rewards)-posNum-posNum]\n",
    "    choiceLarge=rewards_index[-posNum-posNum:-posNum]\n",
    "    print('choiceSmall :',choiceSmall)\n",
    "    print('choiceLarge :',choiceLarge)\n",
    "    largeIndex = np.random.choice(choiceLarge,size=largeNegNum, replace=False)\n",
    "    smallIndex = np.random.choice(choiceSmall,size=smallNegNum, replace=False)\n",
    "    posIndex=rewards_index[-posNum:]\n",
    "    print('smallIndex :',smallIndex)\n",
    "    print('largeIndex :',largeIndex)\n",
    "    print('posIndex :',posIndex)\n",
    "    print(type(posIndex),type(smallIndex),type(largeIndex),)\n",
    "    allIndex=posIndex+list(smallIndex)+list(largeIndex)\n",
    "    print('allIndex :',allIndex)\n",
    "\n",
    "\n",
    "#     p=pSmall+pLarge\n",
    "#     print('p :',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tensors= np.random.rand(10, 2)\n",
    "query_selected=query_tensors[allIndex,:]\n",
    "response_tensors=np.random.rand(10, 2)\n",
    "response_selected=response_tensors[allIndex,:]\n",
    "rewards=np.random.rand(10, 2)\n",
    "rewards_selected=rewards[allIndex,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 6, 2, 8, 1, 5, 0]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allIndex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33898512, 0.17949026],\n",
       "       [0.1709866 , 0.46345098],\n",
       "       [0.60825287, 0.59665541],\n",
       "       [0.33940382, 0.48954894],\n",
       "       [0.05037006, 0.69909807],\n",
       "       [0.94412352, 0.74999925],\n",
       "       [0.87457296, 0.94411975],\n",
       "       [0.23074234, 0.7649117 ]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23074234, 0.7649117 ],\n",
       "       [0.94412352, 0.74999925],\n",
       "       [0.33940382, 0.48954894],\n",
       "       [0.33898512, 0.17949026],\n",
       "       [0.1709866 , 0.46345098],\n",
       "       [0.87457296, 0.94411975],\n",
       "       [0.60825287, 0.59665541],\n",
       "       [0.78364425, 0.5000263 ],\n",
       "       [0.05037006, 0.69909807],\n",
       "       [0.9923964 , 0.26726254]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : -6.477573394775391\n",
      "10 -6.121445655822754\n",
      "20 : -5.9216203689575195\n",
      "30 -5.825961589813232\n",
      "40 : -5.745595932006836\n",
      "50 : -5.6540647\n",
      "60 : -5.5162353515625\n",
      "70 : -5.320635795593262\n",
      "80 : -4.403904438018799\n",
      "82 : -3.534460449218751\n",
      "83 : -2.7513177394866943\n",
      "84 : 0.10986618995668951\n",
      "85 : 3.8011064529418945\n",
      "90 : 4.911001682281494\n",
      "100 : 5.585479736328125\n"
     ]
    }
   ],
   "source": [
    "a = np.array(scores)\n",
    "print('0 :', np.percentile(a, 0))\n",
    "print('10', np.percentile(a, 10))\n",
    "print('20 :', np.percentile(a, 20))\n",
    "print('30', np.percentile(a, 30))\n",
    "print('40 :', np.percentile(a, 40))\n",
    "print('50 :', np.median(a))\n",
    "print('60 :', np.percentile(a, 60))\n",
    "print('70 :', np.percentile(a, 70))\n",
    "print('80 :', np.percentile(a, 80))\n",
    "print('82 :', np.percentile(a, 82))\n",
    "print('83 :', np.percentile(a, 83))\n",
    "print('84 :', np.percentile(a, 84))\n",
    "print('85 :', np.percentile(a, 85))\n",
    "print('90 :', np.percentile(a, 90))\n",
    "print('100 :', np.percentile(a, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#策略 保留所有>=0的例子，然后从所有<0的例子里采样相同size的数据\n",
    "# np.random.seed(0)\n",
    "# np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\n",
    "# array([2, 3, 0])\n",
    "p = np.array([0.1, 0.0, 0.5, 0.4])\n",
    "index = np.random.choice([0, 1, 2, 3],size=3, replace=False,p = p.ravel())\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 22)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_f1_acc),len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375673884196611"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(eval_f1_acc)/len(eval_f1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'Training Loss': 0.2644691658988595,\n",
       "  'Valid. Loss': 0.15922592911043798,\n",
       "  'Valid. Accur.': 0.9431818181818182,\n",
       "  'Training Time': '0:04:30',\n",
       "  'Validation Time': '0:00:09'},\n",
       " {'epoch': 2,\n",
       "  'Training Loss': 0.10734188155736774,\n",
       "  'Valid. Loss': 0.11009371190712872,\n",
       "  'Valid. Accur.': 0.9673295454545454,\n",
       "  'Training Time': '0:04:32',\n",
       "  'Validation Time': '0:00:10'},\n",
       " {'epoch': 3,\n",
       "  'Training Loss': 0.05057939318264835,\n",
       "  'Valid. Loss': 0.04962746564739395,\n",
       "  'Valid. Accur.': 0.9872159090909091,\n",
       "  'Training Time': '0:04:36',\n",
       "  'Validation Time': '0:00:10'},\n",
       " {'epoch': 4,\n",
       "  'Training Loss': 0.033867501211352645,\n",
       "  'Valid. Loss': 0.042183015490247104,\n",
       "  'Valid. Accur.': 0.9900568181818182,\n",
       "  'Training Time': '0:04:36',\n",
       "  'Validation Time': '0:00:09'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'epoch': 1,\n",
    "  'Training Loss': 1.0033449912071228,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:23',\n",
    "  'Validation Time': '0:00:10'},\n",
    " {'epoch': 2,\n",
    "  'Training Loss': 0.9981416406631469,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:26',\n",
    "  'Validation Time': '0:00:10'},\n",
    " {'epoch': 3,\n",
    "  'Training Loss': 0.9991825561523437,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:24',\n",
    "  'Validation Time': '0:00:10'},\n",
    " {'epoch': 4,\n",
    "  'Training Loss': 0.9964690442085267,\n",
    "  'Valid. Loss': 0.9445437436754053,\n",
    "  'Valid. Accur.': 0.1434659090909091,\n",
    "  'Training Time': '0:04:27',\n",
    "  'Validation Time': '0:00:10'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=0, test_size=0.2,\n",
      "            train_size=None)\n",
      "TRAIN: [  369  3199 13563 ... 12122  3547 11232] TEST: [ 4646  9359  4122 ...  9726  3141 12199]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "data=pd.read_csv('/data/hdj/data/CodeBERT/trainAll_0206.csv')\n",
    "X = np.array(data['content'])\n",
    "y = np.array(data['label'])\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "print(split )       # doctest: +ELLIPSIS\n",
    "\n",
    "# StratifiedShuffleSplit(n_splits=3, random_state=0, ...)\n",
    "for train_index, test_index in split.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "train=pd.DataFrame()\n",
    "train['content']=X_train\n",
    "train['label']=y_train\n",
    "train.to_csv('/data/hdj/data/CodeBERT/train_0206.csv',index=False)\n",
    "val=pd.DataFrame()\n",
    "val['content']=X_test\n",
    "val['label']=y_test\n",
    "val.to_csv('/data/hdj/data/CodeBERT/val_0206.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转变最初的txt格式到csv格式\n",
    "for path in ['/home/hdj/data/CodeBERT/eval.csv']:#,'/home/hdj/data/CodeBERT/train.csv'\n",
    "    with open(path) as f_in:\n",
    "        contents=[]\n",
    "        labels=[]\n",
    "        for line in f_in:\n",
    "\n",
    "            line=line.replace(',',' ')\n",
    "            line=line.split('||')\n",
    "\n",
    "            left=line[0].strip()\n",
    "            right=line[1].strip()\n",
    "    #         print(left,label)\n",
    "            contents.append(left)\n",
    "            labels.append(right)\n",
    "        data=pd.DataFrame()\n",
    "        data['content']=contents\n",
    "        data['label']=labels\n",
    "        data.to_csv('/home/hdj/data/CodeBERT/eval_hdj.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
